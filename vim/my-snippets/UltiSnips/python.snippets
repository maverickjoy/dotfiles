snippet tfimp "Tensorflow trainers" b
import numpy as np
import tensorflow as tf
import os, glob, gzip, time
import gensim
import logging

from datetime import datetime
from collections import OrderedDict
from collections import namedtuple
flags = tf.flags

flags.DEFINE_integer("use_gpu", ${1:3}, "GPU to use")
flags.DEFINE_float("gpu_fraction", ${2:1.0}, "Fraction of GPU memory to be allocated")
flags.DEFINE_boolean("allow_gpu_growth", ${3:True}, "If you want to allow memory growth in gpu, when needed")
flags.DEFINE_integer("seed", 1122, "Default seed to set")
flags.DEFINE_string("log_path", "${4:log_path_directory}", "Logger path")

FLAGS = flags.FLAGS
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]=str(FLAGS.use_gpu)
np.random.seed(FLAGS.seed)

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s")

file_handler = logging.FileHandler(os.path.join(${5:args.logfolder}, ${6:args.logfile}))
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(stream_handler)

def main():
	gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_fraction)
	config = tf.ConfigProto(gpu_options=gpu_options)
	config.gpu_options.allow_growth = FLAGS.allow_gpu_growth

	saver = tf.train.Saver(max_to_keep=None)
	merged = tf.summary.merge_all()
	val_writer = tf.summary.FileWriter(os.path.join(FLAGS.log_path, "val"))
	train_writer = tf.summary.FileWriter(os.path.join(FLAGS.log_path, "train"))

	with tf.Session(config=config) as sess:
		sess.run(tf.global_variables_initializer())
		train_writer.add_graph(sess.graph)

		print "TRAINING STARTED..."

		for i in range(${7:epochs}):
			batch = ${8:Dataset.next_batch}


			train_writer.add_summary()
			save_file_path = os.path.join(FLAGS.log_path, "checkpoint", "${9:model_name}")
			saver.save(sess, save_file_path, global_step = ${10:i})

if __name__ == "__main__":
	main()
endsnippet

snippet tfArch "Tensorflow Architecture basic" b
import math

import numpy as np
import tensorflow as tf
import os, glob, gzip, time
import gensim
import logging

from datetime import datetime
from collections import OrderedDict
from collections import namedtuple

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s")

# Update the line below
file_handler = logging.FileHandler(os.path.join(args.logfolder, args.logfile))
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(stream_handler)

class ${1:ARCHITECTURE}:

	OPTIMIZERS = {
			"adam":tf.train.AdamOptimizer,
			"adadelta":tf.train.AdadeltaOptimizer,
			"rmsprop":tf.train.RMSPropOptimizer,
			"sgd":tf.train.GradientDescentOptimizer
	}

	def __init__(self, config):
		# Fill this
		self.build_model()

	def initialize_placeholders(self):
		pass

	def build_model(self):
		logger.info("Building Model ...")
		self.initialize_placeholder()
		pass

	def init_loss(self):
		pass

	def init_optimizer(self):
		logger.info("Setting optimizer")
		trainable_params = tf.trainable_variables()
		self.opt = self.optimizer(learning_rate=self.learning_rate)

		grads = tf.gradients(self.loss, trainable_params)
		clip_grads, _ = tf.clip_by_global_norm(grads, self.max_grad_norm)

		self.updates = self.opt.apply_gradients(zip(clip_grads, trainable_params),global_step=self.global_step)

	def save(self, sess, path, var_list=None, global_step=None):
		saver = tf.train.Saver(var_list)
		save_path = saver.save(sess, save_path=path, global_step=global_step)
		logger.info("Model saved at {}".format(save_path))

	def restore(self, sess, path, var_list=None):
		saver = tf.train.Saver(var_list)
		saver.restore(sess, save_path=path)
		logger.info("Model restored from {}".format(path))

	def train(self, sess, ${2:inputs, outputs}):
		input_feed = self.check_feeds($2)
		output_feed = [${3:outputs}]
		# Keep prob dropout
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def eval(self, sess, ${4:inputs, outputs}):
		input_feed = self.check_feeds($4)
		output_feed = [${5:outputs}]
		# No dropout here
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def predict(self, sess, ${6:inputs}):
		input_feed = self.check_feeds($6)
		output_feed = [${7:logits}]
		# No dropout
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def check_feeds(self, ${8:inputs}):
		# Sanity check for input data like shape, etc.
		input_feed = {}

		# Fill input_feed
		input_feed[${9:Someplaceholder.name}] = ${10:inputs}

		return input_feed
endsnippet


snippet tfplace "Placeholder" b
${1:Var_name} = tf.placeholder(tf.${2:float32}, (None, ${3:300}), name="$1")
endsnippet

snippet tfns "Namescope" b
with tf.name_scope("${1:name}"):
	${2:something_here"}
endsnippet

snippet tfsc "Scalar summary" b
tf.summary.scalar("${1:loss}", $1)
endsnippet

snippet tfhist "Histogram summary" b
tf.summary.histogram(${1:parameter}, $1)
endsnippet

snippet tfvar "tf Variable" b
${1:W} = tf.Variable(${2:tf.random_normal()}, ${3:start_end}, name="W_{${4:1}}")
endsnippet

snippet tfrun "session runner" b
${1:vars} = sess.run([${2:fetch_var}], feed_dict={${3:feed_dictionary}})
endsnippet

snippet tfsumadd "Add summary" b
${1:writer_file}.add_summary(summary,${2:i})
endsnippet

snippet loggert "Add logger statements" b
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s")

file_handler = logging.FileHandler(os.path.join(${1:args.logfolder}, ${2:args.logfile}))
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(stream_handler)
endsnippet
